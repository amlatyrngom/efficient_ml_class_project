{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoQuant: Investigating W4A4 Quantization on OPT-6.7B\n",
    "\n",
    "### Amadou Ngom, Sylvia Zhang, Bowen Zhu, Qihang Chen\n",
    "#### 6.5940 TinyML final project\n",
    "\n",
    "In this notebook, we use OPT-6.7B model to demonstrate the prospects and limitations of W4A4 quantization via combining SmoothQuant and AWQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- smoothquant\n",
    "- PyTorch\n",
    "- Transformers\n",
    "\n",
    "smoothquant and awq should be installed from submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\"\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_125m_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "opt_6_7b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/\"\n",
    "opt_13b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-13b/snapshots/e515202d1e7750da62d245fbccb2723b9c1790f5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fp16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m repo_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m short_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt-125m\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mreport_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshort_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshort_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/efficient_ml_class_project/nanoquant/investigate.py:516\u001b[0m, in \u001b[0;36mreport_sweep\u001b[0;34m(short_model_name, save_dir)\u001b[0m\n\u001b[1;32m    514\u001b[0m baselines \u001b[38;5;241m=\u001b[39m make_baselines()\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m baseline \u001b[38;5;129;01min\u001b[39;00m baselines:\n\u001b[0;32m--> 516\u001b[0m     base_result \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    518\u001b[0m setups \u001b[38;5;241m=\u001b[39m make_setups()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fp16'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "# Force reimport of the module\n",
    "importlib.reload(importlib.import_module(\"nanoquant.investigate\"))\n",
    "importlib.reload(importlib.import_module(\"smoothquant.fake_quant\"))\n",
    "\n",
    "from nanoquant.investigate import sweep, report_sweep, Investigation\n",
    "repo_dir = \".\"\n",
    "short_model_name = \"opt-125m\"\n",
    "# report_sweep(short_model_name=short_model_name, save_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 11:00:51 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d4f2da146b4030b2b78561d4697644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "# Force reimport of the module\n",
    "importlib.reload(importlib.import_module(\"nanoquant.investigate\"))\n",
    "importlib.reload(importlib.import_module(\"smoothquant.fake_quant\"))\n",
    "\n",
    "from nanoquant.investigate import sweep, report_sweep, Investigation\n",
    "\n",
    "repo_dir = \".\"\n",
    "short_model_name = \"opt-125m\"\n",
    "#sweep(short_model_name=short_model_name, repo_dir=repo_dir, save_dir=\".\")\n",
    "#report_sweep(short_model_name=short_model_name, save_dir=\".\")\n",
    "\n",
    "\n",
    "n_bits = 8\n",
    "q_group_size = 0 # 0 means no grouping\n",
    "q_protect = False # False means no protection\n",
    "q_protection_scale = 0.0 # 0.0 means mixed-precision. >= 1.0 means actual scale up/down.\n",
    "q_protection_ratio = 0.01 # 0.01 means 1% of the weights are protected.\n",
    "q_smoothing_strength = 0.5\n",
    "\n",
    "investigation = Investigation(\n",
    "    short_model_name=short_model_name,\n",
    "    local_model_path=opt_125m_model_path,\n",
    "    local_files_only=True,\n",
    "    repo_dir=repo_dir,\n",
    "    n_bits=n_bits,\n",
    "    q_group_size=q_group_size,\n",
    "    q_protect=q_protect,\n",
    "    q_protection_scale=q_protection_scale,\n",
    "    q_protection_ratio=q_protection_ratio,\n",
    "    q_smoothing_strength=q_smoothing_strength\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ Investigate\n",
    "short_model_name = \"opt-125m\"\n",
    "repo_dir = \"llm-awq\"\n",
    "awq_zoo = \"mit-han-lab/awq-model-zoo\"\n",
    "awq_pt_name = f\"llm-awq/awq_cache/{short_model_name}-w4-g128.pt\"\n",
    "\n",
    "from awq.quantize.pre_quant import apply_awq\n",
    "import torch\n",
    "awq_results = torch.load(awq_pt_name, map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': False, 'q_protection_scale': -1.0, 'q_protection_ratio': -1.0, 'q_smoothing_strength': 0.5}\n",
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': True, 'q_protection_scale': 0.0, 'q_protection_ratio': 0.0, 'q_smoothing_strength': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from nanoquant.investigate import sweep, report_sweep, make_setups\n",
    "\n",
    "setups = make_setups()\n",
    "print(setups[0])\n",
    "print(setups[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base fp16 Model Evaluation\n",
    "First, let us evaluate the base fp16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n",
      "Done making base model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 19.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Result: 27.56900978088379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_res = investigation.evaluate_base_model(perp=True)\n",
    "print(f\"Base Result: {base_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-bit Quantization\n",
    "Let us evaluate previous approaches for 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n",
      "Done making base model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:06<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Result: 27.811603546142578\n"
     ]
    }
   ],
   "source": [
    "quantized_res = investigation.evaluate_base_quantized_model(perp=True)\n",
    "print(f\"Quantized Result: {quantized_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n",
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth Quantized Result: 27.627756118774414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "smooth_quantized_res = investigation.evaluate_base_smooth_model(perp=True)\n",
    "print(f\"Smooth Quantized Result: {smooth_quantized_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W4A4 Quantization Variants\n",
    "The following setup-sweep code evaluates different approaches of W4A4 quantization that we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_name(setup):\n",
    "    n_bits = setup[\"n_bits\"]\n",
    "    base_name = f\"W{n_bits}A{n_bits}\"\n",
    "    q_group_size = setup[\"q_group_size\"]\n",
    "    if q_group_size > 0:\n",
    "        base_name += f\" G{q_group_size}\"\n",
    "    q_protect = setup[\"q_protect\"]\n",
    "    if q_protect:\n",
    "        q_protection_scale = setup[\"q_protection_scale\"]\n",
    "        q_protection_ratio = setup[\"q_protection_ratio\"]\n",
    "        with_act = \"Act\" if q_protection_ratio > 1e-5 else \"NoAct\"\n",
    "        if q_protection_scale > 1e-5:\n",
    "            base_name += f\" AWQ-Scaled-{with_act}\"\n",
    "        else:\n",
    "            base_name += f\" AWQ-Mixed-{with_act}\"\n",
    "    return base_name\n",
    "\n",
    "def make_baselines():\n",
    "    return [\"fp16\", \"awq\", \"smoothquant\", \"smoothquant-g\", \"w4a4\", \"smooth-w4a4\", \"w8a8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:37:06 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128: 36.21949005126953\n",
      "Running setup Smooth W4A4 G128\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128: 32.42618179321289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:37:06 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 29.848878860473633\n",
      "Running setup Smooth W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Computing scales after AWQ...\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 30.360246658325195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=opt_125m_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from transformers.models.opt.modeling_opt import (\n",
    "    OPTAttention,\n",
    "    OPTDecoderLayer,\n",
    "    OPTForCausalLM,\n",
    ")\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.fake_quant import WQAQLinear, quantize_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we simulate the 8-bit dynamic per-tensor weight and activation quantization with FP16, i.e., fake quantization. We have implemented the real 8-bit quantization with INT8 CUTLASS GEMM kernels for both PyTorch and FasterTransformer. Please stay tuned for the release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:28:24 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e890c15b0a8414783cf98a0c028a78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#model_name = \"facebook/opt-125m\"\n",
    "model_name\n",
    "#model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "#acc_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "acc_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "#perp_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "perp_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "#acc_dataset = load_dataset(\"lambada\", split=\"validation[:40]\")\n",
    "#perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "cache_dir = \"/state/partition1/user/zzhang1/cache/huggingface/datasets\"\n",
    "acc_dataset_name = f\"{cache_dir}/lambada\"\n",
    "#acc_dataset = load_dataset(acc_dataset_name)\n",
    "n_samples = 40\n",
    "acc_dataset = load_dataset(\"lambada\", split=f\"validation[:{n_samples}]\", cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "perp_dataset_name = f\"{cache_dir}/wikitext\"\n",
    "#perp_dataset = load_dataset(perp_dataset_name)\n",
    "perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test', cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "acc_evaluator = AccuracyEvaluator(acc_dataset, acc_tokenizer, device)\n",
    "perp_evaluator = PerplexityEvaluator(perp_dataset, perp_tokenizer, device, n_samples=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
