{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmoothQuant on OPT-13B\n",
    "\n",
    "### Guangxuan Xiao\\*, Ji Lin\\*, Mickael Seznec, Julien Demouth, Song Han\n",
    "\n",
    "In this notebook, we use OPT-13B model to demonstrate SmoothQuant can use 8-bit for both weights and activations to achieve the same accuracy as FP16 models. Unlike previous method [[Dettmers *et al.*, 2022]](https://arxiv.org/abs/2208.07339), SmoothQuant enables fully INT8 GEMMs for linear layers and does not require high precision numbers to represent outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates SmoothQuant on OPT-13B in consideration of the user's resouce constraints. We have tested SmoothQuant on up to 176 billion parameter models (OPT-175B, BLOOM-176B, GLM-130B). You can also adjust the model name to validate SmoothQuant on other models. `../act_scales/` provides the activation channel scales for OPT and BLOOM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- smoothquant\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\"\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_125m_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "opt_6_7b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/\"\n",
    "opt_13b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-13b/snapshots/e515202d1e7750da62d245fbccb2723b9c1790f5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:22:59 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "# Force reimport of the module\n",
    "importlib.reload(importlib.import_module(\"nanoquant.investigate\"))\n",
    "importlib.reload(importlib.import_module(\"smoothquant.fake_quant\"))\n",
    "\n",
    "from nanoquant.investigate import sweep, report_sweep, Investigation\n",
    "\n",
    "repo_dir = \".\"\n",
    "short_model_name = \"opt-125m\"\n",
    "#sweep(short_model_name=short_model_name, repo_dir=repo_dir, save_dir=\".\")\n",
    "#report_sweep(short_model_name=short_model_name, save_dir=\".\")\n",
    "\n",
    "\n",
    "n_bits = 8\n",
    "q_group_size = 0 # 0 means no grouping\n",
    "q_protect = False # False means no protection\n",
    "q_protection_scale = 0.0 # 0.0 means mixed-precision. >= 1.0 means actual scale up/down.\n",
    "q_protection_ratio = 0.01 # 0.01 means 1% of the weights are protected.\n",
    "q_smoothing_strength = 0.5\n",
    "\n",
    "investigation = Investigation(\n",
    "    short_model_name=short_model_name,\n",
    "    local_model_path=opt_125m_model_path,\n",
    "    local_files_only=True,\n",
    "    repo_dir=repo_dir,\n",
    "    n_bits=n_bits,\n",
    "    q_group_size=q_group_size,\n",
    "    q_protect=q_protect,\n",
    "    q_protection_scale=q_protection_scale,\n",
    "    q_protection_ratio=q_protection_ratio,\n",
    "    q_smoothing_strength=q_smoothing_strength\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': False, 'q_protection_scale': -1.0, 'q_protection_ratio': -1.0, 'q_smoothing_strength': 0.5}\n",
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': True, 'q_protection_scale': 0.0, 'q_protection_ratio': 0.0, 'q_smoothing_strength': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from nanoquant.investigate import sweep, report_sweep, make_setups\n",
    "\n",
    "setups = make_setups()\n",
    "print(setups[0])\n",
    "print(setups[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n",
      "Done making base model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]2024-12-11 14:05:43.514563: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-11 14:05:43.528357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-11 14:05:43.544094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-11 14:05:43.548933: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 14:05:43.560578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 14:05:44.386484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "evaluating...: 100%|██████████| 40/40 [00:08<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Result: 27.56900978088379\n"
     ]
    }
   ],
   "source": [
    "base_res = investigation.evaluate_base_model(perp=True)\n",
    "print(f\"Base Result: {base_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n",
      "Done making base model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:06<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Result: 27.811603546142578\n"
     ]
    }
   ],
   "source": [
    "quantized_res = investigation.evaluate_base_quantized_model(perp=True)\n",
    "print(f\"Quantized Result: {quantized_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n",
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth Quantized Result: 27.627756118774414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "smooth_quantized_res = investigation.evaluate_base_smooth_model(perp=True)\n",
    "print(f\"Smooth Quantized Result: {smooth_quantized_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_name(setup):\n",
    "    n_bits = setup[\"n_bits\"]\n",
    "    base_name = f\"W{n_bits}A{n_bits}\"\n",
    "    q_group_size = setup[\"q_group_size\"]\n",
    "    if q_group_size > 0:\n",
    "        base_name += f\" G{q_group_size}\"\n",
    "    q_protect = setup[\"q_protect\"]\n",
    "    if q_protect:\n",
    "        q_protection_scale = setup[\"q_protection_scale\"]\n",
    "        q_protection_ratio = setup[\"q_protection_ratio\"]\n",
    "        with_act = \"Act\" if q_protection_ratio > 1e-5 else \"NoAct\"\n",
    "        if q_protection_scale > 1e-5:\n",
    "            base_name += f\" AWQ-Scaled-{with_act}\"\n",
    "        else:\n",
    "            base_name += f\" AWQ-Mixed-{with_act}\"\n",
    "    return base_name\n",
    "\n",
    "def make_baselines():\n",
    "    return [\"fp16\", \"awq\", \"smoothquant\", \"smoothquant-g\", \"w4a4\", \"smooth-w4a4\", \"w8a8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:22:59 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:15<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128: 36.21949005126953\n",
      "Running setup Smooth W4A4 G128\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:04<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128: 32.42618179321289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:22:59 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:23<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 29.848878860473633\n",
      "Running setup Smooth W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n",
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:18<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 30.167171478271484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=opt_125m_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from transformers.models.opt.modeling_opt import (\n",
    "    OPTAttention,\n",
    "    OPTDecoderLayer,\n",
    "    OPTForCausalLM,\n",
    ")\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.fake_quant import WQAQLinear, quantize_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we simulate the 8-bit dynamic per-tensor weight and activation quantization with FP16, i.e., fake quantization. We have implemented the real 8-bit quantization with INT8 CUTLASS GEMM kernels for both PyTorch and FasterTransformer. Please stay tuned for the release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**In this demo, we have simplified the evaluation by using the first 1,000 samples from the LAMBADA dataset's validation set. We employ the \"Last Token Prediction Accuracy\" as our evaluation metric. This approximate evaluation is intended for demonstration purposes, providing simple but meaningful comparisons of relative performance between methods. For a more strict assessment, we recommend using the [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness) to obtain the \"Last Word Prediction Accuracy\" for the LAMBADA dataset, which is the reported metric in our paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityEvaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        testenc = self.dataset\n",
    "        nsamples = self.n_samples\n",
    "        model = model.eval()\n",
    "\n",
    "        nlls = []\n",
    "        for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "            batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n",
    "\n",
    "class AccuracyEvaluator:\n",
    "    def __init__(self, dataset, tokenizer, device):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples[\"text\"])\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        for batch in self.dataset:\n",
    "            input_ids = batch[\"input_ids\"].to(self.device).unsqueeze(0)\n",
    "            label = input_ids[:, -1]\n",
    "            outputs = model(input_ids)\n",
    "            last_token_logits = outputs.logits[:, -2, :]\n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "        acc = hit / total\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:28:24 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e890c15b0a8414783cf98a0c028a78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#model_name = \"facebook/opt-125m\"\n",
    "model_name\n",
    "#model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "#acc_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "acc_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "#perp_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "perp_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "#acc_dataset = load_dataset(\"lambada\", split=\"validation[:40]\")\n",
    "#perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "cache_dir = \"/state/partition1/user/zzhang1/cache/huggingface/datasets\"\n",
    "acc_dataset_name = f\"{cache_dir}/lambada\"\n",
    "#acc_dataset = load_dataset(acc_dataset_name)\n",
    "n_samples = 40\n",
    "acc_dataset = load_dataset(\"lambada\", split=f\"validation[:{n_samples}]\", cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "perp_dataset_name = f\"{cache_dir}/wikitext\"\n",
    "#perp_dataset = load_dataset(perp_dataset_name)\n",
    "perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test', cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "acc_evaluator = AccuracyEvaluator(acc_dataset, acc_tokenizer, device)\n",
    "perp_evaluator = PerplexityEvaluator(perp_dataset, perp_tokenizer, device, n_samples=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the performance of the original FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 15/15 [00:00<00:00, 23.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model (fp16) result: 29.11785888671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = OPTForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "res_fp16 = perp_evaluator.evaluate(model_fp16)\n",
    "print(f\"Original model (fp16) result: {res_fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then quantize the model to W8A8 and check the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 15/15 [00:02<00:00,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive W4A4 quantized model result: 35.55803680419922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = OPTForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "n_bits = 4\n",
    "q_group_size = 128 # 0 means no group\n",
    "q_protect = True # False means no protection\n",
    "q_protection_ratio = 0.01 # 0.01 means 1% of the weights are protected.\n",
    "q_protection_scale = 0.0 # 0.0 mixed-precision. >1.0 means scale up/down.\n",
    "q_name = f\"W{n_bits}A{n_bits}\"\n",
    "q_model = quantize_opt(\n",
    "    model_fp16,\n",
    "    n_bits=n_bits,\n",
    "    q_group_size=q_group_size,\n",
    "    q_protect=q_protect,\n",
    "    q_protection_ratio=q_protection_ratio,\n",
    "    q_protection_scale=q_protection_scale,\n",
    ")\n",
    "q_res = perp_evaluator.evaluate(q_model)\n",
    "print(f\"Naive {q_name} quantized model result: {q_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): WALinear(768, 768, bias=True, weight_quant=protected_group_quant_128, act_quant=protected_group_quant_128, output_quant=protected_group_quant_128)\n",
      "            (v_proj): WALinear(768, 768, bias=True, weight_quant=protected_group_quant_128, act_quant=protected_group_quant_128, output_quant=protected_group_quant_128)\n",
      "            (q_proj): WALinear(768, 768, bias=True, weight_quant=protected_group_quant_128, act_quant=protected_group_quant_128, output_quant=protected_group_quant_128)\n",
      "            (out_proj): WALinear(768, 768, bias=True, weight_quant=protected_group_quant_128, act_quant=protected_group_quant_128, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): WALinear(768, 3072, bias=True, weight_quant=protected_group_quant_128, act_quant=protected_group_quant_128, output_quant=None)\n",
      "          (fc2): WALinear(3072, 768, bias=True, weight_quant=protected_group_quant_128, act_quant=protected_group_quant_128, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(q_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a significant accuracy drop. This is consistent with LLM.int8()'s finding: when the model size increases larger than 6.7B, systematic outliers will emerge in activations, which makes fully INT8 quantization impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's smooth the model, quantize it, and check the performance! In `../act_scales`, we provide the activation scales for OPT and BLOOM models. You can also use this notebook to test quantizing those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 15/15 [00:01<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed W4A4 quantized model result: 33.575801849365234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = OPTForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "scales_path = \"smoothquant/act_scales/opt-125m.pt\"\n",
    "act_scales = torch.load(scales_path)\n",
    "smooth_lm(model, act_scales, 0.5)\n",
    "q_model_smooth = quantize_opt(\n",
    "    model,\n",
    "    n_bits=n_bits,\n",
    "    q_group_size=q_group_size,\n",
    "    q_protect=q_protect,\n",
    "    q_protection_ratio=q_protection_ratio,\n",
    "    q_protection_scale=q_protection_scale,\n",
    ")\n",
    "q_res_smooth = perp_evaluator.evaluate(q_model_smooth)\n",
    "print(f\"Smoothed {q_name} quantized model result: {q_res_smooth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the smoothed model has the same accuracy as the FP16 model. This is because SmoothQuant smooths the outliers in activations and moves the quantization difficulty from activations to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
