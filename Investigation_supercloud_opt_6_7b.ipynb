{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoQuant: Investigating W4A4 Quantization on OPT-6.7B\n",
    "\n",
    "### Amadou Ngom, Sylvia Zhang, Bowen Zhu, Qihang Chen\n",
    "#### 6.5940 TinyML final project\n",
    "\n",
    "In this notebook, we use OPT-6.7B model to demonstrate the prospects and limitations of W4A4 quantization via combining SmoothQuant and AWQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Accelerate\n",
    "\n",
    "smoothquant and awq should be installed from submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\"\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_125m_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "opt_6_7b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/\"\n",
    "opt_13b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-13b/snapshots/e515202d1e7750da62d245fbccb2723b9c1790f5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# Force reimport of the module\n",
    "importlib.reload(importlib.import_module(\"nanoquant.investigate\"))\n",
    "importlib.reload(importlib.import_module(\"smoothquant.fake_quant\"))\n",
    "\n",
    "from nanoquant.investigate import sweep, report_sweep, Investigation\n",
    "repo_dir = \".\"\n",
    "short_model_name = \"opt-6.7b\"\n",
    "#report_sweep(short_model_name=short_model_name, save_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:22:59 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "# Force reimport of the module\n",
    "importlib.reload(importlib.import_module(\"nanoquant.investigate\"))\n",
    "importlib.reload(importlib.import_module(\"smoothquant.fake_quant\"))\n",
    "\n",
    "from nanoquant.investigate import sweep, report_sweep, Investigation\n",
    "\n",
    "repo_dir = \".\"\n",
    "short_model_name = \"opt-6.7b\"\n",
    "#sweep(short_model_name=short_model_name, repo_dir=repo_dir, save_dir=\".\")\n",
    "#report_sweep(short_model_name=short_model_name, save_dir=\".\")\n",
    "\n",
    "\n",
    "n_bits = 8\n",
    "q_group_size = 0 # 0 means no grouping\n",
    "q_protect = False # False means no protection\n",
    "q_protection_scale = 0.0 # 0.0 means mixed-precision. >= 1.0 means actual scale up/down.\n",
    "q_protection_ratio = 0.01 # 0.01 means 1% of the weights are protected.\n",
    "q_smoothing_strength = 0.5\n",
    "\n",
    "# Sanity check - Investigation can be successfully constructed, models and datasets are loaded\n",
    "investigation = Investigation(\n",
    "    short_model_name=short_model_name,\n",
    "    local_model_path=opt_6_7b_model_path,\n",
    "    local_files_only=True,\n",
    "    repo_dir=repo_dir,\n",
    "    n_bits=n_bits,\n",
    "    q_group_size=q_group_size,\n",
    "    q_protect=q_protect,\n",
    "    q_protection_scale=q_protection_scale,\n",
    "    q_protection_ratio=q_protection_ratio,\n",
    "    q_smoothing_strength=q_smoothing_strength\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ Investigate\n",
    "short_model_name = \"opt-6.7b\"\n",
    "repo_dir = \"llm-awq\"\n",
    "awq_zoo = \"mit-han-lab/awq-model-zoo\"\n",
    "# sanity check: awq parameters exist. Download from awq-model-zoo if this step fails.\n",
    "awq_pt_name = f\"llm-awq/awq_cache/{short_model_name}-w4-g128.pt\"\n",
    "\n",
    "from awq.quantize.pre_quant import apply_awq\n",
    "import torch\n",
    "awq_results = torch.load(awq_pt_name, map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': False, 'q_protection_scale': -1.0, 'q_protection_ratio': -1.0, 'q_smoothing_strength': 0.5}\n",
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': True, 'q_protection_scale': 0.0, 'q_protection_ratio': 0.0, 'q_smoothing_strength': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from nanoquant.investigate import sweep, report_sweep, make_setups, make_setup\n",
    "\n",
    "setups = make_setups()\n",
    "print(setups[0])\n",
    "print(setups[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base fp16 Model Evaluation\n",
    "First, let us evaluate the base fp16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dd6390f3a34c8ca9c3afecca06c350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]2024-12-14 19:01:13.690778: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-14 19:01:13.704777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-14 19:01:13.721676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-14 19:01:13.726730: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-14 19:01:13.739602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 19:01:14.455844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "evaluating...: 100%|██████████| 40/40 [00:32<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Result: 10.673927307128906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_res = investigation.evaluate_base_model(perp=True)\n",
    "print(f\"Base Result: {base_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-bit Quantization\n",
    "Let us evaluate previous approaches for 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3013daf04e50459398f7d2430da8a5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:32<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Result: 21.003461837768555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "quantized_res = investigation.evaluate_base_quantized_model(perp=True)\n",
    "print(f\"Quantized Result: {quantized_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4c012db6024024b36cae61a39dad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:32<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth Quantized Result: 10.707098007202148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "smooth_quantized_res = investigation.evaluate_base_smooth_model(perp=True)\n",
    "print(f\"Smooth Quantized Result: {smooth_quantized_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W4A4 Quantization Variants\n",
    "The following setup-sweep code evaluates different approaches of W4A4 quantization that we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:48:52 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583e3168cbf24eb1829f9ab0861e4a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]2024-12-13 00:30:45.883462: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-13 00:30:45.895939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-13 00:30:45.911513: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-13 00:30:45.916326: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-13 00:30:45.927739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 00:30:46.670970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "evaluating...: 100%|██████████| 40/40 [00:38<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128: 12.023670196533203\n",
      "Running setup Smooth W4A4 G128\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231c122427d343078e780e97ba1c2f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:36<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128: 11.20273494720459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:48:52 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307c6029dc9b430cb251b884569d2c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:36<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 10.866781234741211\n",
      "Running setup Smooth W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ab084ea4d740f385d1198cf049c179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Computing scales after AWQ...\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:36<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 11.00145435333252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=opt_6_7b_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing strength investigation\n",
    "We can see that applying SmoothQuant on top of AWQ group quantization gives a slight degradation, although the SmoothQuant parameters we use here are calculated based on the AWQ quantized model. Next, we investigate different smoothing strengths to see if it makes a difference. The following two cells set up the parameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_name(setup):\n",
    "    n_bits = setup[\"n_bits\"]\n",
    "    base_name = f\"W{n_bits}A{n_bits}\"\n",
    "    q_group_size = setup[\"q_group_size\"]\n",
    "    if q_group_size > 0:\n",
    "        base_name += f\" G{q_group_size}\"\n",
    "    q_protect = setup[\"q_protect\"]\n",
    "    if q_protect:\n",
    "        q_protection_scale = setup[\"q_protection_scale\"]\n",
    "        q_protection_ratio = setup[\"q_protection_ratio\"]\n",
    "        with_act = \"Act\" if q_protection_ratio > 1e-5 else \"NoAct\"\n",
    "        if q_protection_scale > 1e-5:\n",
    "            base_name += f\" AWQ-Scaled-{with_act}\"\n",
    "        else:\n",
    "            base_name += f\" AWQ-Mixed-{with_act}\"\n",
    "    return base_name\n",
    "\n",
    "def make_baselines():\n",
    "    return [\"fp16\", \"awq\", \"smoothquant\", \"smoothquant-g\", \"w4a4\", \"smooth-w4a4\", \"w8a8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alpha_sweep_setups():\n",
    "    q_group_size = 128\n",
    "    n_bits = 4\n",
    "    setups = []\n",
    "    q_smoothing_strength = 0.6\n",
    "    # With protection\n",
    "    q_protect = True\n",
    "    # Weight-only protection.\n",
    "    q_protection_scale = 0.0\n",
    "    q_protection_ratio = 0.0\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_smoothing_strength = 0.5\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_smoothing_strength = 0.4\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_smoothing_strength = 0.3\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_protection_scale = 0.0\n",
    "    #q_protection_ratio = 0.03\n",
    "    #setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    return setups\n",
    "\n",
    "setups = make_alpha_sweep_setups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:48:52 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546daba7822a4dabb36603df486d7a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:36<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 10.866781234741211\n",
      "Running setup Smooth W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ff63ad2dfc426882963d1cec41003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Computing scales after AWQ...\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:36<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 10.904909133911133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:48:52 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 10.866781234741211\n",
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 10.904909133911133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:48:52 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 10.866781234741211\n",
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 10.904909133911133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Thu Dec 12 23:48:52 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 12 11:00:56 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 10.866781234741211\n",
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 10.904909133911133\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=opt_6_7b_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation protection investigation\n",
    "We can see that different smoothing scales do not make a difference. Next, we investigate whether salient weight protection makes an impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mixed_sweep_setups():\n",
    "    q_group_size = 128\n",
    "    n_bits = 4\n",
    "    setups = []\n",
    "    q_smoothing_strength = 0.6\n",
    "    # With protection\n",
    "    q_protect = True\n",
    "    # Weight-only protection.\n",
    "    q_protection_scale = 0.3\n",
    "    q_smoothing_strength = 0.5\n",
    "    q_protection_ratio = 0.01\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_protection_scale = 0.3\n",
    "    q_protection_ratio = 0.03\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    return setups\n",
    "\n",
    "setups = make_mixed_sweep_setups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:22:59 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Scaled-Act\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5e848d91444902b3d8dbcab7fcd4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:37<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Scaled-Act: 10.863935470581055\n",
      "Running setup Smooth W4A4 G128 AWQ-Scaled-Act\n",
      "Making base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b212dec46e4d09a068ae7327d02bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Computing scales after AWQ...\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:37<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Scaled-Act: 10.981032371520996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:22:59 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Scaled-Act: 10.863935470581055\n",
      "Smooth W4A4 G128 AWQ-Scaled-Act: 10.981032371520996\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=opt_6_7b_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Compared to Smooth + AWQ combined no activation protection, this version boosts the perplexity on `wikitext2` by another 0.02. Compared to the fp16 model, combining all the approaches give us a 0.31 perplexity increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model_name = \"facebook/opt-125m\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel_name\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#acc_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m acc_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Misc.\n",
    "from datasets import load_dataset\n",
    "\n",
    "#model_name = \"facebook/opt-125m\"\n",
    "model_name\n",
    "#model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "#acc_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "acc_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "#perp_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "perp_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "#acc_dataset = load_dataset(\"lambada\", split=\"validation[:40]\")\n",
    "#perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "cache_dir = \"/state/partition1/user/zzhang1/cache/huggingface/datasets\"\n",
    "acc_dataset_name = f\"{cache_dir}/lambada\"\n",
    "#acc_dataset = load_dataset(acc_dataset_name)\n",
    "n_samples = 40\n",
    "acc_dataset = load_dataset(\"lambada\", split=f\"validation[:{n_samples}]\", cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "perp_dataset_name = f\"{cache_dir}/wikitext\"\n",
    "#perp_dataset = load_dataset(perp_dataset_name)\n",
    "perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test', cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "acc_evaluator = AccuracyEvaluator(acc_dataset, acc_tokenizer, device)\n",
    "perp_evaluator = PerplexityEvaluator(perp_dataset, perp_tokenizer, device, n_samples=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then quantize the model to W8A8 and check the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
