{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoQuant: Investigating W4A4 Quantization on Llama2 -7B\n",
    "\n",
    "### Amadou Ngom, Sylvia Zhang, Bowen Zhu, Qihang Chen\n",
    "#### 6.5940 TinyML final project\n",
    "\n",
    "In this notebook, we use Llama-2-7b model to demonstrate the prospects and limitations of W4A4 quantization via combining SmoothQuant and AWQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Accelerate\n",
    "\n",
    "smoothquant and awq should be installed from submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\"\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME=\"/state/partition1/user/zzhang1/cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_125m_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "opt_6_7b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0/\"\n",
    "opt_13b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-13b/snapshots/e515202d1e7750da62d245fbccb2723b9c1790f5/\"\n",
    "llama_2_7b_model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# Force reimport of the module\n",
    "importlib.reload(importlib.import_module(\"nanoquant.investigate\"))\n",
    "importlib.reload(importlib.import_module(\"smoothquant.fake_quant\"))\n",
    "\n",
    "from nanoquant.investigate import sweep, report_sweep, Investigation\n",
    "repo_dir = \".\"\n",
    "short_model_name = \"llama-2-7b\"\n",
    "# report_sweep(short_model_name=short_model_name, save_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 15:53:47 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf15c00972c4e95bbed52e40535881e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "# Force reimport of the module\n",
    "importlib.reload(importlib.import_module(\"nanoquant.investigate\"))\n",
    "importlib.reload(importlib.import_module(\"smoothquant.fake_quant\"))\n",
    "\n",
    "from nanoquant.investigate import sweep, report_sweep, Investigation\n",
    "\n",
    "repo_dir = \".\"\n",
    "short_model_name = \"llama-2-7b\"\n",
    "#sweep(short_model_name=short_model_name, repo_dir=repo_dir, save_dir=\".\")\n",
    "#report_sweep(short_model_name=short_model_name, save_dir=\".\")\n",
    "\n",
    "\n",
    "n_bits = 8\n",
    "q_group_size = 0 # 0 means no grouping\n",
    "q_protect = False # False means no protection\n",
    "q_protection_scale = 0.0 # 0.0 means mixed-precision. >= 1.0 means actual scale up/down.\n",
    "q_protection_ratio = 0.01 # 0.01 means 1% of the weights are protected.\n",
    "q_smoothing_strength = 0.5\n",
    "\n",
    "# Sanity check - Investigation can be successfully constructed, models and datasets are loaded\n",
    "investigation = Investigation(\n",
    "    short_model_name=short_model_name,\n",
    "    local_model_path=llama_2_7b_model_path,\n",
    "    local_files_only=True,\n",
    "    repo_dir=repo_dir,\n",
    "    n_bits=n_bits,\n",
    "    q_group_size=q_group_size,\n",
    "    q_protect=q_protect,\n",
    "    q_protection_scale=q_protection_scale,\n",
    "    q_protection_ratio=q_protection_ratio,\n",
    "    q_smoothing_strength=q_smoothing_strength\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ Investigate\n",
    "short_model_name = \"llama-2-7b\"\n",
    "repo_dir = \"llm-awq\"\n",
    "awq_zoo = \"mit-han-lab/awq-model-zoo\"\n",
    "# sanity check: awq parameters exist. Download from awq-model-zoo if this step fails.\n",
    "awq_pt_name = f\"llm-awq/awq_cache/{short_model_name}-w4-g128.pt\"\n",
    "\n",
    "from awq.quantize.pre_quant import apply_awq\n",
    "import torch\n",
    "awq_results = torch.load(awq_pt_name, map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': False, 'q_protection_scale': -1.0, 'q_protection_ratio': -1.0, 'q_smoothing_strength': 0.5}\n",
      "{'n_bits': 4, 'q_group_size': 128, 'q_protect': True, 'q_protection_scale': 0.0, 'q_protection_ratio': 0.0, 'q_smoothing_strength': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from nanoquant.investigate import sweep, report_sweep, make_setups, make_setup\n",
    "\n",
    "setups = make_setups()\n",
    "print(setups[0])\n",
    "print(setups[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base fp16 Model Evaluation\n",
    "First, let us evaluate the base fp16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688da44c7e704a96bab1b15023ed5abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...:   0%|          | 0/40 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "evaluating...: 100%|██████████| 40/40 [00:24<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Result: 5.8229875564575195\n"
     ]
    }
   ],
   "source": [
    "base_res = investigation.evaluate_base_model(perp=True)\n",
    "print(f\"Base Result: {base_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-bit Quantization\n",
    "Let us evaluate previous approaches for 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c51288519cb4e78b08207dc01a62e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:27<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Result: 5.931421279907227\n"
     ]
    }
   ],
   "source": [
    "quantized_res = investigation.evaluate_base_quantized_model(perp=True)\n",
    "print(f\"Quantized Result: {quantized_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c08c18a226345abba3f0f242514a3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:27<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth Quantized Result: 5.944703578948975\n"
     ]
    }
   ],
   "source": [
    "smooth_quantized_res = investigation.evaluate_base_smooth_model(perp=True)\n",
    "print(f\"Smooth Quantized Result: {smooth_quantized_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W4A4 Quantization Variants\n",
    "The following setup-sweep code evaluates different approaches of W4A4 quantization that we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10363db4278241da930645597879a16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128: 6.4598469734191895\n",
      "Running setup Smooth W4A4 G128\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e37c7f9a854af88c7cb9ac28cce060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... False\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:30<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128: 6.469829559326172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228d6eb7df1c45c8bbe782df61d44349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:31<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 6.089395523071289\n",
      "Running setup Smooth W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da682b5d3d04b9db5b5c85bc2b54d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Computing scales after AWQ...\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:31<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 6.082112789154053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=llama_2_7b_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing strength investigation\n",
    "We can see that applying SmoothQuant on top of AWQ group quantization gives a very tiny improvement, although the SmoothQuant parameters we use here are calculated based on the AWQ quantized model. Next, we investigate different smoothing strengths to see if it makes a difference. The following two cells set up the parameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_name(setup):\n",
    "    n_bits = setup[\"n_bits\"]\n",
    "    base_name = f\"W{n_bits}A{n_bits}\"\n",
    "    q_group_size = setup[\"q_group_size\"]\n",
    "    if q_group_size > 0:\n",
    "        base_name += f\" G{q_group_size}\"\n",
    "    q_protect = setup[\"q_protect\"]\n",
    "    if q_protect:\n",
    "        q_protection_scale = setup[\"q_protection_scale\"]\n",
    "        q_protection_ratio = setup[\"q_protection_ratio\"]\n",
    "        with_act = \"Act\" if q_protection_ratio > 1e-5 else \"NoAct\"\n",
    "        if q_protection_scale > 1e-5:\n",
    "            base_name += f\" AWQ-Scaled-{with_act}\"\n",
    "        else:\n",
    "            base_name += f\" AWQ-Mixed-{with_act}\"\n",
    "    return base_name\n",
    "\n",
    "def make_baselines():\n",
    "    return [\"fp16\", \"awq\", \"smoothquant\", \"smoothquant-g\", \"w4a4\", \"smooth-w4a4\", \"w8a8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alpha_sweep_setups():\n",
    "    q_group_size = 128\n",
    "    n_bits = 4\n",
    "    setups = []\n",
    "    q_smoothing_strength = 0.6\n",
    "    # With protection\n",
    "    q_protect = True\n",
    "    # Weight-only protection.\n",
    "    q_protection_scale = 0.0\n",
    "    q_protection_ratio = 0.0\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_smoothing_strength = 0.5\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_smoothing_strength = 0.4\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_smoothing_strength = 0.3\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_protection_scale = 0.0\n",
    "    #q_protection_ratio = 0.03\n",
    "    #setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    return setups\n",
    "\n",
    "setups = make_alpha_sweep_setups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f7cee243d742b1a7e9cfdb5aeb7be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:31<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 6.089395523071289\n",
      "Running setup Smooth W4A4 G128 AWQ-Mixed-NoAct\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0294515b5004e2ba50e136531e9a8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Computing scales after AWQ...\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:31<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 6.077132701873779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 6.089395523071289\n",
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 6.077132701873779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 6.089395523071289\n",
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 6.077132701873779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Mixed-NoAct: 6.089395523071289\n",
      "Smooth W4A4 G128 AWQ-Mixed-NoAct: 6.077132701873779\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=llama_2_7b_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation protection investigation\n",
    "We can see that different smoothing scales do not make a difference. Next, we investigate whether salient weight protection makes an impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mixed_sweep_setups():\n",
    "    q_group_size = 128\n",
    "    n_bits = 4\n",
    "    setups = []\n",
    "    q_smoothing_strength = 0.6\n",
    "    # With protection\n",
    "    q_protect = True\n",
    "    # Weight-only protection.\n",
    "    q_protection_scale = 0.3\n",
    "    q_smoothing_strength = 0.5\n",
    "    q_protection_ratio = 0.01\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    q_protection_scale = 0.3\n",
    "    q_protection_ratio = 0.03\n",
    "    setups.append(make_setup(n_bits, q_group_size, q_protect, q_protection_scale, q_protection_ratio, q_smoothing_strength))\n",
    "    return setups\n",
    "\n",
    "setups = make_mixed_sweep_setups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup W4A4 G128 AWQ-Scaled-Act\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff9ca7c4ee74d7ca02f0b98ad088da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:32<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Scaled-Act: 6.079077243804932\n",
      "Running setup Smooth W4A4 G128 AWQ-Scaled-Act\n",
      "Making base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df3e762cf1d432da245b10b6b7b5aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making base model.\n",
      "Applying AWQ...\n",
      "Done applying AWQ.\n",
      "Computing scales after AWQ...\n",
      "Smoothing model...\n",
      "Done smoothing model.\n",
      "Quantizing model...\n",
      "Quantizing model... True\n",
      "Done quantizing model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:32<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth W4A4 G128 AWQ-Scaled-Act: 6.06747579574585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Sat Dec 14 16:03:28 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A4 G128 AWQ-Scaled-Act: 6.079077243804932\n",
      "Smooth W4A4 G128 AWQ-Scaled-Act: 6.06747579574585\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "save_dir = \".\"\n",
    "perp = True\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "result_file = f\"{save_dir}/results_{short_model_name}.pkl\"\n",
    "#if os.path.exists(result_file):\n",
    "#    with open(result_file, \"rb\") as f:\n",
    "#        results = pkl.load(f)\n",
    "#else:\n",
    "results = {}\n",
    "for setup in setups:\n",
    "        setup_key = str(setup)\n",
    "        base_expt_name = setup_name(setup)\n",
    "        if setup_key in results and base_expt_name != \"W4A4 G128\":\n",
    "            print(f\"Setup {base_expt_name} already run. Results={results[setup_key]['q_res']}, SmoothResults={results[setup_key]['q_smooth_res']}\")\n",
    "            continue\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        investigation = Investigation(\n",
    "            short_model_name=short_model_name,\n",
    "            repo_dir=repo_dir,\n",
    "            local_model_path=llama_2_7b_model_path,\n",
    "            local_files_only=True,\n",
    "            **setup)\n",
    "        simple_expt_name = f\"{base_expt_name}\"\n",
    "        if simple_expt_name not in results:\n",
    "            print(f\"Running setup {base_expt_name}\")\n",
    "            q_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=False)\n",
    "            results[simple_expt_name] = q_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_res = results[simple_expt_name]\n",
    "        print(f\"{simple_expt_name}: {q_res}\")\n",
    "        # Smoothed model\n",
    "        smooth_expt_name = f\"Smooth {base_expt_name}\"\n",
    "        if smooth_expt_name not in results:\n",
    "            print(f\"Running setup {smooth_expt_name}\")\n",
    "            q_smooth_res = investigation.evaluate_setup_model(perp=perp, apply_smooth=True)\n",
    "            results[smooth_expt_name] = q_smooth_res\n",
    "            with open(result_file, \"wb\") as f:\n",
    "                pkl.dump(results, f)\n",
    "        else:\n",
    "            q_smooth_res = results[smooth_expt_name]\n",
    "        print(f\"{smooth_expt_name}: {q_smooth_res}\")\n",
    "        res = {\n",
    "            \"setup\": setup,\n",
    "            \"q_res\": q_res,\n",
    "            \"q_smooth_res\": q_smooth_res,\n",
    "        }\n",
    "        results[setup_key] = res\n",
    "        # Checkpointing\n",
    "        with open(result_file, \"wb\") as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Compared to Smooth + AWQ combined no activation protection, this version boosts the perplexity on `wikitext2` by another 0.02. Compared to the 5.82 base fp16 model perplexity, combining all the approaches give us a 0.24 perplexity increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since lambada couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /state/partition1/user/zzhang1/cache/huggingface/datasets/lambada/plain_text/0.0.0/5953bd97664b64b95754f299b2309ecfbfbe81b9 (last modified on Wed Dec 11 00:28:24 2024).\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /state/partition1/user/zzhang1/cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Dec 11 00:23:01 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e890c15b0a8414783cf98a0c028a78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#model_name = \"facebook/opt-125m\"\n",
    "model_name\n",
    "#model_path = \"/state/partition1/user/zzhang1/cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/\"\n",
    "#acc_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "acc_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "#perp_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "perp_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "#acc_dataset = load_dataset(\"lambada\", split=\"validation[:40]\")\n",
    "#perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "cache_dir = \"/state/partition1/user/zzhang1/cache/huggingface/datasets\"\n",
    "acc_dataset_name = f\"{cache_dir}/lambada\"\n",
    "#acc_dataset = load_dataset(acc_dataset_name)\n",
    "n_samples = 40\n",
    "acc_dataset = load_dataset(\"lambada\", split=f\"validation[:{n_samples}]\", cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "perp_dataset_name = f\"{cache_dir}/wikitext\"\n",
    "#perp_dataset = load_dataset(perp_dataset_name)\n",
    "perp_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test', cache_dir=\"/state/partition1/user/zzhang1/cache/huggingface/datasets/\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "acc_evaluator = AccuracyEvaluator(acc_dataset, acc_tokenizer, device)\n",
    "perp_evaluator = PerplexityEvaluator(perp_dataset, perp_tokenizer, device, n_samples=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
